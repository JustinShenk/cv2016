{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osnabrück University - Computer Vision (Winter Term 2016/17) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 09: Local Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, January 29, 2017**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (Local features – 8p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Explain in your own words what *local features* are how they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "solution": true
   },
   "source": [
    "Subsets of the image that can be used for classification and detection.\n",
    "\n",
    "When working with local features, we do not consider the picture as a whole. Instead we select a few small image patches and work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Explain the idea of SIFT and describe the main steps. How do these steps contribute the the properties of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "solution": true
   },
   "source": [
    "* detect extrema in scale space (search over all pixels on all scales)\n",
    "* keypoint detection (select stable keypoints): multiscale representation by convolution with gaussians with different widths. scale of an object detected via maximum in scale space. fe the extrema of the second derivative. in the end remove unstable keypoints \n",
    "* direction detection (based on gradient) histogram of directions is computed from the gradient directions around the keypoints (x, y, scale, direction) \n",
    "* keypoint descriptor (transform to a standardized descriptor) combine directions within 4x4 patches around keypoints. these are the feature descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Explain the idea of rectangle features. How are they computed? What is an integral image?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "solution": true
   },
   "source": [
    "Rectangular features are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Compare SIFT and rectangle features. Name pros and cons."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "solution": true
   },
   "source": [
    "FIXME: change cell type to 'Markdown' and put your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (SIFT – 12p)\n",
    "\n",
    "Implementation of SIFT keypoint detection. Use the image `img.png` for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Calculate the scale space for an image $I$ (no built-ins allowed). Calculate 4 octaves with 5 scales each. For the calculation of one octave proceed as follows:\n",
    "Smooth the image with $\\sigma_0 = 0.5$ with a kernel size of $\\operatorname{ceil}(4\\cdot\\sigma_0)$. This is the first image of the octave. For the calculation of the other scale images ($i = 1,\\ldots,4$), the value of $\\sigma$ is calculated as $\\sigma_i=k_i\\cdot\\sigma_0$ with $k = 2$ and kernel size $\\operatorname{ceil}(4\\cdot\\sigma_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "M(x,y,sigma) = G(x,y,sigma) * g(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def change(x = 2):    \n",
    "    if x:\n",
    "        x -=1\n",
    "        print(x)\n",
    "        change(x)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "change(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinshenk/anaconda3/envs/cvloop/lib/python3.5/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data can not convert to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-da1ab4d8b008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justinshenk/anaconda3/envs/cvloop/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3030\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justinshenk/anaconda3/envs/cvloop/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justinshenk/anaconda3/envs/cvloop/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   4920\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   4921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4922\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4923\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justinshenk/anaconda3/envs/cvloop/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    447\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    448\u001b[0m                 not np.can_cast(self._A.dtype, np.float)):\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data can not convert to float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         if (self._A.ndim not in (2, 3) or\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data can not convert to float"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import misc\n",
    "\n",
    "img = misc.imread('img.png')\n",
    "img = img[:,:,2]\n",
    "imgs = {}\n",
    "\n",
    "def gaussian(img, times=2):\n",
    "    # Specify number of times to recursively perform Gaussian.\n",
    "    imgs[times] = np.zeros((img.shape))\n",
    "    \n",
    "    rows,cols = imgs[times].shape\n",
    "    \n",
    "    # Gaussian kernel.\n",
    "    gaussian_kernel_1 = [1,2,1,2,4,2,1,2,1]    \n",
    "    for row in range(rows-1):\n",
    "        for col in range(cols-1):\n",
    "            patch_list = [img[row-1,col-1], img[row-1,col], img[row-1,col+1], img[row,col-1], img[row,col], \\\n",
    "                          img[row,col+1], img[row+1,col-1], img[row+1,col], img[row+1,col+1]]\n",
    "            patch_product = [a*b for a,b in zip(gaussian_kernel_1, patch_list)]\n",
    "            average = sum(patch_product) / 16 \n",
    "            imgs[times][row,col] = average\n",
    "            \n",
    "    if times:\n",
    "        times -= 1\n",
    "        plt.figure()\n",
    "        plt.imshow(imgs[times+1],plt.cm.gray)\n",
    "        imgs[times] = gaussian(imgs[times+1], times)    \n",
    "    else:\n",
    "        return\n",
    "\n",
    "plt.imshow(img,plt.cm.gray)\n",
    "plt.figure()\n",
    "plt.imshow(gaussian(img),plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [2, 3]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import misc\n",
    "\n",
    "# img = misc.imread('img.png')\n",
    "# img.shape\n",
    "a = np.arange(8).reshape(2,2,2)\n",
    "a[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Calculate the Difference of Gaussian images from the scale space for each octave\n",
    "($DoG_{i−1} = G(x,y,\\sigma_i) − G(x,y,σ_{i−1})$), where $G(x,y,\\sigma_i)$ is the Gaussian smoothed image with kernel $\\sigma_i$. The result will be 4 DoG images per octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e931a2a3b881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg_scaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_scaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Detect extrema in the DoG images. Check every pixel in the DoG images and compare it to all its neighbors (surrounding, as well as those neighbors in the adjacent DoG images). This is a total of 26 comparisons. If the pixel is greater than or less than all its neighbors, it is a possible keypoint. This check is not performed for the top- and lowermost DoG images within each octave, because there are not enough neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: put your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Actually, we would have to calculate subpixel locations now, but we will omit that part. Instead we will remove keypoints that lie on edges. This procedure is similar to that of the Harris corner detector, however, in the following the Hessian matrix is used. Calculate the Hessian matrix of the DoG images on which (possible) keypoints were found. \n",
    "If $\\operatorname{Tr}(H)^2/\\operatorname{det}(H) > (r + 1)^2/r$, with $\\operatorname{Tr}(H) = D_{xx} + D_{yy}$ and $\\operatorname{det}(H) = D_{xx} D_{yy} − D_{xy}$ then the keypoint is rejected. Find a suitable value for $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: put your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Keypoints are also rejected if their magnitude in the DoG image is less than a threshold\n",
    "(find a good threshold yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: put your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Display the remaining keypoints on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME: put your code here!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
